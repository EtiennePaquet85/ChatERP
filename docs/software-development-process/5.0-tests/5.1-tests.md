# ✅ ChatERP – Tests

---

## 1. 🎯 Objectif du document

Ce document présente la stratégie de tests du prototype **ChatERP**, conformément au chapitre *Software Testing* du SWEBOK.  
Il décrit les niveaux de tests prévus, leurs objectifs, les types de tests utilisés ainsi que les liens avec les composants du système.

---

## 2. 🧪 Niveaux de tests

| Niveau              | Description                                                                         | Objectifs principaux                                               |
|---------------------|-------------------------------------------------------------------------------------|--------------------------------------------------------------------|
| Tests unitaires     | Tests de chaque composant logiciel de manière isolée (fonctions, classes, services) | Vérifier que chaque unité fonctionne conformément à sa spécification |
| Tests d’intégration | Vérification des interactions entre composants (frontend/backend/database)          | Assurer une intégration correcte des modules entre eux            |
| Tests système       | Vérification du comportement global de l’application à partir d’un scénario complet | Valider que le système répond aux exigences fonctionnelles         |

---

## 3. 🧩 Objectifs de tests

| ID         | Objectif                       | Description                                                                                    | Alignement             |
|------------|--------------------------------|------------------------------------------------------------------------------------------------|------------------------|
| TST-OBJ-01 | Validation unitaire            | Vérifier que chaque composant fonctionne correctement de façon isolée                          | REQ-NF-02, REQ-NF-03   |
| TST-OBJ-02 | Cohérence inter-composants     | S’assurer que les interactions entre composants (API, base de données, UI) sont fonctionnelles | REQ-NF-02, REQ-CONT-03 |
| TST-OBJ-03 | Couverture fonctionnelle       | Tester les cas d’utilisation critiques dans leur intégralité                                   | REQ-F-01, REQ-F-04     |
| TST-OBJ-04 | Non-régression automatisée     | Détecter automatiquement toute régression après une modification                               | REQ-NF-03, REQ-NF-04   |
| TST-OBJ-05 | Robustesse du système          | S’assurer que les erreurs sont bien gérées sans provoquer de plantage                          | REQ-NF-04              |
| TST-OBJ-06 | Conformité aux exigences       | Vérifier que les comportements respectent les exigences du document de vision                  | REQ-NF-03, REQ-F-02    |
| TST-OBJ-07 | Intégration continue des tests | Intégrer tous les tests dans un pipeline de CI/CD pour des validations systématiques           | REQ-NF-03, REQ-CONT-02 |

---

## 4. 🧰 Types de tests

| Type                    | Applicabilité                             | Outils/Méthodes                                |
|-------------------------|-------------------------------------------|------------------------------------------------|
| Tests automatisés       | Tests unitaires et partiels d’intégration | xUnit (.NET), Postman, scripts TypeScript      |
| Tests manuels           | Tests système, parcours utilisateur       | Scénarios documentés, validation sur prototype |
| Tests de non-régression | Avant chaque mise à jour                  | Réexécution des jeux de tests automatisés      |

---

## 5. 🔄 Processus de tests

1. **Préparation** : rédaction des cas de tests selon les exigences et cas d’utilisation.  
2. **Exécution** : manuelle ou automatisée selon le niveau.  
3. **Suivi des résultats** : journalisation des anomalies, couverture des exigences.  
4. **Correction et re-test** : cycle itératif jusqu’à obtention de résultats conformes.

---

## 6. 📄 Journal des tests

| ID Test       | Résultat attendu                                                 | Statut       |
|---------------|------------------------------------------------------------------|--------------|
| TST-UC01A-01  | L’employé est ajouté avec un ID unique                           | En attente   |
| TST-UC01A-02  | Saisie invalide rejetée avec message d’erreur                    | En attente   |
| TST-UC01A-03  | Employé avec courriel existant refusé                            | En attente   |
| TST-UC01A-04  | Erreur base de données correctement gérée                        | En attente   |
| TST-UC01B-01  | Photo JPEG/PNG acceptée et stockée                               | En attente   |
| TST-UC01B-02  | Fichier invalide rejeté avec message d’erreur                    | En attente   |
| TST-UC01B-03  | URL de la photo retournée et enregistrée                         | En attente   |
| TST-UC02A-01  | Liste des employés chargée et affichée sans erreur               | En attente   |
| TST-UC02A-02  | Affichage ordonné par identifiant                                | En attente   |
| TST-UC02B-01  | Informations complètes de l’employé affichées                    | En attente   |
| TST-UC02B-02  | Message d’erreur si employé introuvable                          | En attente   |
| TST-UC03-01   | Modification d’un champ sauvegardée correctement                 | En attente   |
| TST-UC03-02   | Validation des données modifiées (ex : email valide)             | En attente   |
| TST-UC03-03   | Rejet si données invalides                                       | En attente   |
| TST-UC03-04   | Gestion des conflits simultanés                                  | En attente   |
| TST-UC03-05   | Modification de photo prise en charge                            | En attente   |
| TST-UC04-01   | Suppression simple avec confirmation utilisateur                 | En attente   |
| TST-UC04-02   | Refus de suppression si dépendances critiques détectées          | En attente   |
| TST-UC04-03   | Message d’erreur en cas d’échec de suppression                   | En attente   |

---

## 7. 📌 Couverture des cas d’utilisation

| Cas d’utilisation                   | Couverture testée                                        | Niveau                         | ID de test                   |
|-------------------------------------|----------------------------------------------------------|--------------------------------|------------------------------|
| `UCS-UC-01a` – Créer employé        | Création valide, invalide, duplication                   | Unitaire, intégration, système | TST-UC01A-01 à TST-UC01A-04  |
| `UCS-UC-01b` – Téléverser une photo | Formats supportés, erreurs de transfert, URL enregistrée | Intégration, système           | TST-UC01B-01 à TST-UC01B-03  |
| `UCS-UC-02a` – Liste employés       | Chargement, tri, affichage                               | Intégration, système           | TST-UC02A-01 à TST-UC02A-02  |
| `UCS-UC-02b` – Détails employé      | Affichage complet, employé introuvable                   | Intégration, système           | TST-UC02B-01 à TST-UC02B-02  |
| `UCS-UC-03`  – Modifier employé     | Modifications valides/invalide, conflit, image modifiée  | Unitaire, intégration          | TST-UC03-01 à TST-UC03-05    |
| `UCS-UC-04`  – Supprimer employé    | Suppression simple, dépendance, échec suppression        | Système                        | TST-UC04-01 à TST-UC04-03    |

---

## 8. ⚙️ Procédures de tests

### 8.1 🌐 Systèmes concernés

| Système    | Langage / Framework            | Dossier des tests                            |
|------------|--------------------------------|----------------------------------------------|
| Backend    | C# / .NET 8 + xUnit            | `systems/backend/chaterp-server-tests`       |
| Database   | Python 3.12 + FastAPI + Pytest | `systems/database/chaterp-persistence-tests` |
| Frontend   | TypeScript + React + Vitest    | `systems/frontend/chaterp-web-tests`         |

---

### 8.2 ⚙️ Préparer l’environnement de tests

#### Backend (.NET) – Installer les dépendances

- Terminal :
  ~~~
  cd systems/backend/chaterp-server-tests
  dotnet restore
  ~~~

#### Database (Python/FastAPI) – Préparer l’environnement virtuel et installer les dépendances

- Linux/macOS :
  ~~~
  cd systems/database/chaterp-persistence-tests
  python -m venv .venv
  source .venv/bin/activate
  pip install -r requirements.txt
  ~~~
- Windows PowerShell :
  ~~~
  cd systems/database/chaterp-persistence-tests
  python -m venv .venv
  .venv\Scripts\Activate.ps1
  pip install -r requirements.txt
  ~~~
- Windows Command Prompt (cmd.exe) :
  ~~~
  cd systems/database/chaterp-persistence-tests
  python -m venv .venv
  .venv\Scripts\activate.bat
  pip install -r requirements.txt
  ~~~
- Windows Git Bash (MINGW64) :
  ~~~
  cd systems/database/chaterp-persistence-tests
  python -m venv .venv
  source .venv/Scripts/activate
  pip install -r requirements.txt
  ~~~

#### Frontend (React/Vite) – Installer les dépendances

- Terminal :
  ~~~
  cd systems/frontend/chaterp-web-tests
  npm install
  ~~~

---

### 8.3 🚀 Exécuter les tests (sans coverage)

#### Backend (.NET) – Lancer les tests

- Terminal :
  ~~~
  cd systems/backend/chaterp-server-tests
  dotnet test
  ~~~

#### Database (Python/FastAPI) – Lancer les tests

- Linux/macOS :
  ~~~
  cd systems/database/chaterp-persistence-tests
  source .venv/bin/activate
  make test
  ~~~
- Windows PowerShell :
  ~~~
  cd systems/database/chaterp-persistence-tests
  .venv\Scripts\Activate.ps1
  make test
  ~~~
- Windows Command Prompt (cmd.exe) :
  ~~~
  cd systems/database/chaterp-persistence-tests
  .venv\Scripts\activate.bat
  make test
  ~~~
- Windows Git Bash (MINGW64) :
  ~~~
  cd systems/database/chaterp-persistence-tests
  source .venv/Scripts/activate
  make test
  ~~~

#### Frontend (React/Vite) – Lancer les tests

- Terminal :
  ~~~
  cd systems/frontend/chaterp-web-tests
  npm run test
  ~~~

---

### 8.4 🚀 Exécuter les tests (avec coverage)

#### Backend (.NET) – Lancer les tests avec couverture

- Terminal :
  ~~~
  cd systems/backend/chaterp-server-tests
  dotnet test-coverage
  ~~~

#### Database (Python/FastAPI) – Lancer les tests avec couverture

- Linux/macOS :
  ~~~
  cd systems/database/chaterp-persistence-tests
  source .venv/bin/activate
  make test-coverage
  ~~~
- Windows PowerShell :
  ~~~
  cd systems/database/chaterp-persistence-tests
  .venv\Scripts\Activate.ps1
  make test-coverage
  ~~~
- Windows Command Prompt (cmd.exe) :
  ~~~
  cd systems/database/chaterp-persistence-tests
  .venv\Scripts\activate.bat
  make test-coverage
  ~~~
- Windows Git Bash (MINGW64) :
  ~~~
  cd systems/database/chaterp-persistence-tests
  source .venv/Scripts/activate
  make test-coverage
  ~~~

#### Frontend (React/Vite) – Lancer les tests avec couverture

- Terminal :
  ~~~
  cd systems/frontend/chaterp-web-tests
  npm run test:coverage
  ~~~

---

### 8.5 🧪 Exécuter globalement les tests avec script PowerShell

Pour simplifier l'exécution de tous les tests du projet **ChatERP**, un script unifié est disponible.

📄 **Fichier :**  
`ChatERP/scripts/tests/run-all-tests.ps1`

#### 🧭 Objectif

Ce script PowerShell exécute automatiquement l'ensemble des tests du projet ChatERP : Frontend, Backend, et Database. 
Il synthétise les résultats dans la console avec des messages clairs et colorés.

#### ▶️ Lancement du script depuis la racine du projet :

- Windows PowerShell :
  ~~~
  powershell -ExecutionPolicy Bypass -File .\scripts\tests\run-all-tests.ps1
  ~~~

#### 🔍 Fonctionnement

Le script appelle successivement les sous-scripts suivants :

1. ✅ **Tests Frontend** – `run-frontend-tests.ps1`  
2. ✅ **Tests Backend** – `run-backend-tests.ps1`  
3. ✅ **Tests Database** – `run-database-tests.ps1`  

À chaque étape :

- Le code de sortie (`$LASTEXITCODE`) est vérifié  
- Les erreurs sont capturées et affichées proprement  
- Un résumé global est affiché à la fin :
    - ✅ Tous les tests ont réussi  
    - ❌ Un ou plusieurs tests ont échoué

#### 🔁 Utilisation typique

- Avant un *commit* ou une *pull request*  
- Dans un *pipeline CI/CD*  
- Pour valider rapidement l’intégrité globale du système après modification

#### 📌 Prérequis

- Les scripts suivants doivent être présents et opérationnels : `run-frontend-tests.ps1`, `run-backend-tests.ps1`, `run-database-tests.ps1`
- PowerShell doit être installé (exécutable via `powershell` ou `pwsh`)
- Le script est écrit en **UTF-8** pour un affichage multilingue correct


---

### 8.6 🚀 Exécuter automatiquement les tests lors du déploiement avec `chaterp-compose`

Pour faciliter la construction et le déploiement des images Docker de ChatERP, un outil CLI .NET nommé **`chaterp-compose`** est disponible (voir `6.1-deployment.md`).  
Ce programme orchestre automatiquement l'exécution des tests, la construction des images Docker, leur publication, puis le démarrage des services.

#### 🎯 Objectif principal

L'exécution automatisée des tests sert à :

- **Valider automatiquement la qualité du projet en lançant tous les tests automatisés** (backend, database, frontend) avant toute autre étape.  
- **Empêcher la construction et le déploiement si un ou plusieurs tests échouent**, garantissant ainsi la stabilité des livrables.

#### 📁 Emplacement du fichier

~~~
docker/chaterp-compose/Program.cs
~~~

#### ▶️ Utilisation

Depuis la racine du projet, exécuter :

~~~
dotnet run --project docker/chaterp-compose
~~~

#### 🧪 Étape 1 : Exécution des tests automatisés

Chaque suite de tests est exécutée dans son environnement dédié, selon l’ordre suivant :

~~~
Console.WriteLine("🔹 Tests backend (.NET)...");
if (!RunShellCommand("dotnet test", @"systems\backend\chaterp-server-tests"))
{
    Console.WriteLine("❌ Échec des tests backend.");
    return;
}

Console.WriteLine("🔹 Tests database (Python)...");
if (!RunShellCommand(".venv\\Scripts\\python.exe -m pytest", @"systems\database\chaterp-persistence-tests"))
{
    Console.WriteLine("❌ Échec des tests database.");
    return;
}

Console.WriteLine("🔹 Tests frontend (React/Vitest)...");
if (!RunShellCommand("npm run test", @"systems\frontend\chaterp-web-tests"))
{
    Console.WriteLine("❌ Échec des tests frontend.");
    return;
}
~~~

#### 🛠 Étapes suivantes (si tous les tests réussissent)

En cas de succès, les étapes suivantes sont exécutées automatiquement (voir `6.1-deployment.md` pour plus de détails) :

1. Construction des images Docker  
2. Publication vers le registre Docker  
3. Démarrage des services via Docker Compose

#### 📋 Résumé

Cette automatisation garantit que seuls les builds validés par les tests sont déployés, améliorant la robustesse du processus CI/CD de ChatERP.  
L'exécution est détaillée en console avec affichage clair des erreurs ou succès.

---

## 9. 🔄 Répétition des tests

- Exécuter les tests après chaque modification majeure.  
- En cas d’échec, consulter les logs pour diagnostic.  
- Corriger les anomalies avant de poursuivre le développement.

---

## 10. 📄 Résultats et rapports

- Les résultats s’affichent dans la console avec détails d’exécution, erreurs, et couverture (quand configurée).  
- Pour le backend, la couverture peut être exportée dans des formats compatibles avec les outils d’analyse.  
- Pour la database, `pytest-cov` génère un rapport de couverture.  
- Pour le frontend, Vitest affiche la couverture et les tests passés/échoués.

---

## 11. 🧩 Intégration possible avec Docker / CI

- Les tests peuvent être automatisés dans les pipelines CI/CD.  
- Il est possible de lancer ces commandes dans des conteneurs dédiés, en adaptant les volumes et commandes.  
- Voir la documentation Docker Compose pour intégration complète.

---

## 12. 🔚 Conclusion

La stratégie de tests définie pour **ChatERP** permet de valider la fiabilité, la robustesse et la conformité du système à chaque étape de son évolution.  
En couvrant tous les niveaux (unitaire, intégration, système) et en s’appuyant sur des outils adaptés, elle garantit un haut niveau de qualité logicielle.

Ce cadre de tests est conçu pour être maintenable, extensible et facilement intégrable aux processus de déploiement et de livraison continue.  
Il pourra être ajusté selon les besoins futurs du projet ou l’ajout de nouvelles fonctionnalités.
